\section{Related Work} \label{sec:rel}


\noindent \textbf{Online Review Mining.} With the rapid growth of online e-commerce services, online review mining has become a hot research topic \cite{Pang+Lee:08b}. A large body of these studies focuses on various application tasks based on online reviews \cite{Jindal+Liu:08a,Archak+Ghose+Ipeirotis:07a,Branavan+al:08a,Breck+Choi+Cardie:07a}. In particular, topic models  (e.g., Latent Dirichlet Allocation \cite{blei:jmlr03}) have been widely applied to derive aspects and sentiments from review text, since it is shown to be effective to discover probabilistic clustering of words, termed as \emph{topics}. To characterize sentiments or opinions, many variations based on the standard topic models have been proposed: The final representation forms of sentiments can be global \cite{Mei+al:07a,Hu+Liu:04b} or aspect-specific \cite{zhao-EtAl:2010:EMNLP,xu-cikm-2012}. More directly, online reviews have been leveraged to improve the results of product ranking or recommendation. Liu et al. (\cite{Liu+al:07b}) proposed to use a sentiment model to predict sales performance; while in \cite{mcglohon-icwsm-2010}, composite rating scores were derived from aggregated reviews collected from multiple websites using different statistic- and heuristic-based methods and were subsequently used to rank products and merchants. Ganu et al. (\cite{ganu2009beyond,Ganu:2013:IQP}) derived text-based ratings of item aspects from review text and then grouped similar users together using soft clustering techniques based on the topics and sentiments that appeared in the reviews. In short, the core idea of these studies is to transform opinionated text into sentiment scores, which can be used to rank products.
\\
\\
\noindent \textbf{Representation Learning.} Recently, researches for neural language models \cite{nlm}, such as \texttt{word2vec} \cite{word_to_vector} and \texttt{paragraph2vec} \cite{paragraph_vector}, have generalized the classic $n$-gram language models by using continuous variables to represent words in a vector space and have been successfully applied to latent semantics discovery for NLP tasks. 
More recently, the concept of distributed representations has been extended beyond pure language words to a number of applications, including modeling  phrases \cite{phrases}, relational entities \cite{entity1,entity2}, general text-based attributes \cite{attribute} and descriptive text of images \cite{image}.  Our work is based on the the well-known \texttt{word2vec} \cite{word_to_vector} and \texttt{paragraph2vec} \cite{paragraph_vector} models, which represent words or paragraphs as the embedding vectors in the low-dimensional semantic space. We have made two major extensions based on them: (1) the automatic selection of pseudo labels and (2) the incorporation of pseudo labels in the embedding models.  As will be shown in the experiments, these two extensions lead to significant improvement in the current task. It is noted that the ideas for these extensions can indeed apply to other more complicated neural network models, such as convolution neural networks (CNN) \cite{CNN}.
We will leave it as future work.
