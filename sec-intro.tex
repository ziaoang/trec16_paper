\section{Introduction}
With the rapid development of the microblog, such as Twitter and Weibo,
the information that microblog covered is rather numerous than expected.
To explore user's interests and boost recommendation performance in real-time environment,
TREC first introduced Tweet Timeline Generation (TTG) track in 2014\cite{lin2014overview} and developed it in 2015.
The Real-Time Summarization Track in TREC 2016 is a real-time summarization task broken down into two scenarios,
which is aiming to explore techniques for monitoring streams of social media posts with respect to users' interest profiles.
Different from the last year's Microblog track, it requires a real on-line decision,
which means participating systems need to decide whether or
not push notification for a tweet before seeing the subsequent tweets.
And two scenarios are described as follow:

\begin{itemize}
\item \textbf{Scenario A (push notification):}
Content that is identified as relevant and novel by a system based
on the user's interest profile should be sent to the user in a timely fashion. 
\item \textbf{Scenario B (email digest):}
Participating systems should identify tweets and aggregate them into an email digest.
The email should be periodically sent to a user. Under that circumstances,
users can read a longer story about the contents.
\end{itemize}

In the push notifications scenario, our system requires to ``listen'' to
the Twitter API\footnote{https://github.com/lintool/twitter-tools}
and make real-time push actions for each interest profile.
We design an on-line system which contains three modules:
Filter Module, Judge Module and Submit Module.
When a new tweet $D$ comes, we use Filter Module to remove it
if it has no overlap words with all the interest profiles.
In the Judge Module, we first estimate the relevance score between the tweet
and each interest profile using negative KL-divergence.
A tuned relevance threshold $\alpha$ is utilized to judge whether $D$ and
the interest profile are relevant.
Meanwhile, we keep a push queue for each interest profile.
Then, for every relevant interest profile $Q$, we estimate the novel score by comparing $D$
with previous tweets in its push queue.
Similarly, we use negative KL-divergence and a tuned
novel threshold $\beta$ to decide whether $D$ is a novel one for the interest profile $Q$.
Submit Module is used to submit passed tweets to the Evaluation Broker with the power to 
handle error code from the remote server.
It can also store the push queue for each interest profile and help recover our system
if any crash happens.

In the scenario of email digest, similar with scenario A,
we firstly clean the raw tweets generated from the evaluation period.
To better understand the user intent,
we utilize Google web resource as external evidences to expand original query.
Then, the language model framework is applied to estimate the relevance between given interest profile with candidate tweets.
For each interest profile,
we rank the candidate tweets of every day by the relevance scores which adopt two different smooth methods. 
Once we obtain the ranked tweet list,
we calculate the novelty scores between the candidate tweet with each tweet that has been pushed previously,
the novelty threshold $\gamma$ is used to determine whether the candidate tweet is included in the email digest.
And there are two kinds of strategies to measure the novelty, i.e. negative KL-divergence and Simhash.


